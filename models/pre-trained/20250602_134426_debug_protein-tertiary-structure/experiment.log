INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:: mem (CPU python)=537.00390625MB; mem (CPU total)=72339.56640625MB
INFO:root:############### Starting experiment with config file debug_protein-tertiary-structure.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'protein-tertiary-structure', 'yarin_gal_uci_split_indices': 0, 'max_dataset_size': 1000, 'standardize': True}
INFO:root:After loading the datasets: mem (CPU python)=561.375MB; mem (CPU total)=72589.37890625MB
INFO:root:###1 out of 1 training parameter combinations ###
INFO:root:Training parameters: {'report_every': 5, 'seed': 1234, 'model': 'MLP', 'uncertainty_quantification': 'dropout', 'backbone': 'default', 'batch_size': 64, 'eval_batch_size': 16384, 'n_epochs': 1000, 'early_stopping': 50, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0.0, 'dropout': 0.1, 'hidden_dim': 64, 'n_layers': 2, 'distributional_method': 'mixednormal', 'concat_condition_diffusion': True, 'evaluate': True, 'x_T_sampling_method': 'CARD', 'conditional_free_guidance_training': True, 'regressor': None}
INFO:root:Using split-0 for UCI dataset protein-tertiary-structure
INFO:root:After creating the dataloaders: mem (CPU python)=561.375MB; mem (CPU total)=72602.33203125MB
INFO:root:NumberParameters: 9025
INFO:root:GPU memory allocated: 2097152
INFO:root:After setting up the model: mem (CPU python)=2152.1875MB; mem (CPU total)=68993.453125MB
INFO:root:Training starts now.
INFO:root:[    5] Training loss: 0.68249980, Validation loss: 0.62845103, Validation loss EMA: 0.61798048
INFO:root:[   10] Training loss: 0.61596104, Validation loss: 0.59484299, Validation loss EMA: 0.57978062
INFO:root:[   15] Training loss: 0.59749473, Validation loss: 0.59353191, Validation loss EMA: 0.57292424
INFO:root:[   20] Training loss: 0.58569129, Validation loss: 0.58734067, Validation loss EMA: 0.57156012
INFO:root:[   25] Training loss: 0.57690075, Validation loss: 0.56816252, Validation loss EMA: 0.54870409
INFO:root:[   30] Training loss: 0.56707647, Validation loss: 0.55965755, Validation loss EMA: 0.53705448
INFO:root:[   35] Training loss: 0.55861612, Validation loss: 0.55319202, Validation loss EMA: 0.53248403
INFO:root:[   40] Training loss: 0.55170556, Validation loss: 0.55073565, Validation loss EMA: 0.52592152
INFO:root:[   45] Training loss: 0.54466800, Validation loss: 0.54107946, Validation loss EMA: 0.51938365
INFO:root:[   50] Training loss: 0.53807539, Validation loss: 0.52944281, Validation loss EMA: 0.50738462
INFO:root:[   55] Training loss: 0.53278841, Validation loss: 0.53435795, Validation loss EMA: 0.50933707
INFO:root:[   60] Training loss: 0.52822271, Validation loss: 0.52801083, Validation loss EMA: 0.50050195
INFO:root:[   65] Training loss: 0.52217684, Validation loss: 0.51715807, Validation loss EMA: 0.49441072
INFO:root:[   70] Training loss: 0.51906185, Validation loss: 0.51537663, Validation loss EMA: 0.49237856
INFO:root:[   75] Training loss: 0.51389555, Validation loss: 0.50444136, Validation loss EMA: 0.47879579
INFO:root:[   80] Training loss: 0.50992046, Validation loss: 0.49796109, Validation loss EMA: 0.47139055
INFO:root:[   85] Training loss: 0.50574841, Validation loss: 0.49081313, Validation loss EMA: 0.46305871
INFO:root:[   90] Training loss: 0.50319903, Validation loss: 0.49625654, Validation loss EMA: 0.46894121
INFO:root:[   95] Training loss: 0.49887495, Validation loss: 0.50331783, Validation loss EMA: 0.47718816
INFO:root:[  100] Training loss: 0.49446319, Validation loss: 0.49485006, Validation loss EMA: 0.46163476
INFO:root:[  105] Training loss: 0.49287210, Validation loss: 0.50391705, Validation loss EMA: 0.46926472
INFO:root:[  110] Training loss: 0.49015302, Validation loss: 0.49790831, Validation loss EMA: 0.46698368
INFO:root:[  115] Training loss: 0.48737397, Validation loss: 0.49005602, Validation loss EMA: 0.45860142
INFO:root:[  120] Training loss: 0.48359968, Validation loss: 0.47480243, Validation loss EMA: 0.44646610
INFO:root:[  125] Training loss: 0.48250461, Validation loss: 0.47825981, Validation loss EMA: 0.44590347
INFO:root:[  130] Training loss: 0.47985723, Validation loss: 0.48191258, Validation loss EMA: 0.45207891
INFO:root:[  135] Training loss: 0.47828526, Validation loss: 0.47502553, Validation loss EMA: 0.44134392
INFO:root:[  140] Training loss: 0.47488833, Validation loss: 0.47427053, Validation loss EMA: 0.44090053
INFO:root:[  145] Training loss: 0.47321472, Validation loss: 0.46973829, Validation loss EMA: 0.43779730
INFO:root:[  150] Training loss: 0.47150083, Validation loss: 0.47170662, Validation loss EMA: 0.43824777
INFO:root:[  155] Training loss: 0.47018882, Validation loss: 0.46505734, Validation loss EMA: 0.43352170
INFO:root:[  160] Training loss: 0.46859894, Validation loss: 0.46855800, Validation loss EMA: 0.43312295
INFO:root:[  165] Training loss: 0.46747118, Validation loss: 0.45836872, Validation loss EMA: 0.42754716
INFO:root:[  170] Training loss: 0.46575577, Validation loss: 0.45889448, Validation loss EMA: 0.42273853
INFO:root:[  175] Training loss: 0.46526249, Validation loss: 0.46577321, Validation loss EMA: 0.43201356
INFO:root:[  180] Training loss: 0.46308342, Validation loss: 0.45748973, Validation loss EMA: 0.41898569
INFO:root:[  185] Training loss: 0.46068883, Validation loss: 0.45758355, Validation loss EMA: 0.41986172
INFO:root:[  190] Training loss: 0.46099951, Validation loss: 0.46294132, Validation loss EMA: 0.42353840
INFO:root:[  195] Training loss: 0.45927047, Validation loss: 0.45362933, Validation loss EMA: 0.41698994
INFO:root:[  200] Training loss: 0.45780009, Validation loss: 0.45141241, Validation loss EMA: 0.41706452
INFO:root:[  205] Training loss: 0.45673594, Validation loss: 0.44982226, Validation loss EMA: 0.41617401
INFO:root:[  210] Training loss: 0.45466546, Validation loss: 0.44928628, Validation loss EMA: 0.41402047
INFO:root:[  215] Training loss: 0.45537986, Validation loss: 0.45283023, Validation loss EMA: 0.41800773
INFO:root:[  220] Training loss: 0.45324621, Validation loss: 0.45315473, Validation loss EMA: 0.41648968
INFO:root:[  225] Training loss: 0.45174684, Validation loss: 0.45002580, Validation loss EMA: 0.41356393
INFO:root:[  230] Training loss: 0.45169743, Validation loss: 0.44573554, Validation loss EMA: 0.40944507
INFO:root:[  235] Training loss: 0.45022927, Validation loss: 0.44846497, Validation loss EMA: 0.41080582
INFO:root:[  240] Training loss: 0.44844370, Validation loss: 0.44422143, Validation loss EMA: 0.40757579
INFO:root:[  245] Training loss: 0.44753343, Validation loss: 0.45483332, Validation loss EMA: 0.41919449
INFO:root:[  250] Training loss: 0.44730606, Validation loss: 0.45268790, Validation loss EMA: 0.41309255
INFO:root:[  255] Training loss: 0.44751511, Validation loss: 0.43806121, Validation loss EMA: 0.40101813
INFO:root:[  260] Training loss: 0.44502407, Validation loss: 0.43175952, Validation loss EMA: 0.39418456
INFO:root:[  265] Training loss: 0.44431150, Validation loss: 0.43491916, Validation loss EMA: 0.39856019
INFO:root:[  270] Training loss: 0.44420855, Validation loss: 0.43037469, Validation loss EMA: 0.39652512
INFO:root:[  275] Training loss: 0.44295830, Validation loss: 0.44112404, Validation loss EMA: 0.40151172
INFO:root:[  280] Training loss: 0.44138977, Validation loss: 0.43553901, Validation loss EMA: 0.39811021
INFO:root:[  285] Training loss: 0.44171602, Validation loss: 0.43623247, Validation loss EMA: 0.39856629
INFO:root:[  290] Training loss: 0.44044476, Validation loss: 0.43584037, Validation loss EMA: 0.39853393
INFO:root:[  295] Training loss: 0.43913357, Validation loss: 0.44303095, Validation loss EMA: 0.40573608
INFO:root:[  300] Training loss: 0.43959649, Validation loss: 0.44224293, Validation loss EMA: 0.40033276
INFO:root:[  305] Training loss: 0.43946755, Validation loss: 0.43843499, Validation loss EMA: 0.39991517
INFO:root:[  310] Training loss: 0.43711910, Validation loss: 0.43324936, Validation loss EMA: 0.39724209
INFO:root:[  315] Training loss: 0.43945499, Validation loss: 0.42740590, Validation loss EMA: 0.39063482
INFO:root:[  320] Training loss: 0.43689336, Validation loss: 0.43716829, Validation loss EMA: 0.39748736
INFO:root:[  325] Training loss: 0.43585804, Validation loss: 0.43607664, Validation loss EMA: 0.39220507
INFO:root:[  330] Training loss: 0.43638870, Validation loss: 0.42817082, Validation loss EMA: 0.39016783
INFO:root:[  335] Training loss: 0.43409604, Validation loss: 0.42881073, Validation loss EMA: 0.39018675
INFO:root:[  340] Training loss: 0.43358074, Validation loss: 0.43244876, Validation loss EMA: 0.39101370
INFO:root:[  345] Training loss: 0.43315669, Validation loss: 0.43102033, Validation loss EMA: 0.39297597
INFO:root:[  350] Training loss: 0.43185455, Validation loss: 0.42477122, Validation loss EMA: 0.38343382
INFO:root:[  355] Training loss: 0.43203390, Validation loss: 0.43251540, Validation loss EMA: 0.38979907
INFO:root:[  360] Training loss: 0.43273597, Validation loss: 0.42637604, Validation loss EMA: 0.38797475
INFO:root:[  365] Training loss: 0.43157935, Validation loss: 0.43199955, Validation loss EMA: 0.39006040
INFO:root:[  370] Training loss: 0.43113301, Validation loss: 0.42939435, Validation loss EMA: 0.38508338
INFO:root:[  375] Training loss: 0.42908546, Validation loss: 0.42827078, Validation loss EMA: 0.38568823
INFO:root:[  380] Training loss: 0.42955727, Validation loss: 0.42376004, Validation loss EMA: 0.38358263
INFO:root:[  385] Training loss: 0.42976915, Validation loss: 0.42402550, Validation loss EMA: 0.38422909
INFO:root:[  390] Training loss: 0.42824423, Validation loss: 0.42266264, Validation loss EMA: 0.38389050
INFO:root:[  395] Training loss: 0.42710363, Validation loss: 0.41930174, Validation loss EMA: 0.37920803
INFO:root:[  400] Training loss: 0.42709659, Validation loss: 0.42101463, Validation loss EMA: 0.38246759
INFO:root:[  405] Training loss: 0.42798570, Validation loss: 0.42564369, Validation loss EMA: 0.37967583
INFO:root:[  410] Training loss: 0.42690830, Validation loss: 0.42527863, Validation loss EMA: 0.38408994
INFO:root:[  415] Training loss: 0.42699958, Validation loss: 0.42419221, Validation loss EMA: 0.38157772
INFO:root:[  420] Training loss: 0.42353297, Validation loss: 0.41884101, Validation loss EMA: 0.37773570
INFO:root:[  425] Training loss: 0.42440479, Validation loss: 0.41674824, Validation loss EMA: 0.37942619
INFO:root:[  430] Training loss: 0.42428157, Validation loss: 0.41761300, Validation loss EMA: 0.37657474
INFO:root:[  435] Training loss: 0.42204822, Validation loss: 0.42916059, Validation loss EMA: 0.38230191
INFO:root:[  440] Training loss: 0.42280268, Validation loss: 0.42404019, Validation loss EMA: 0.38136338
INFO:root:[  445] Training loss: 0.42142704, Validation loss: 0.42243100, Validation loss EMA: 0.38041974
INFO:root:[  450] Training loss: 0.42187141, Validation loss: 0.42400660, Validation loss EMA: 0.37806299
INFO:root:[  455] Training loss: 0.42112362, Validation loss: 0.41955590, Validation loss EMA: 0.37897559
INFO:root:[  460] Training loss: 0.42103132, Validation loss: 0.42423700, Validation loss EMA: 0.38149777
INFO:root:[  465] Training loss: 0.42098247, Validation loss: 0.42227138, Validation loss EMA: 0.38054740
INFO:root:[  470] Training loss: 0.42067888, Validation loss: 0.42048454, Validation loss EMA: 0.37834733
INFO:root:[  475] Training loss: 0.42058908, Validation loss: 0.41011740, Validation loss EMA: 0.36596689
INFO:root:[  480] Training loss: 0.41962167, Validation loss: 0.41157938, Validation loss EMA: 0.37090811
INFO:root:[  485] Training loss: 0.41865394, Validation loss: 0.41524678, Validation loss EMA: 0.37117881
INFO:root:[  490] Training loss: 0.41882513, Validation loss: 0.41270069, Validation loss EMA: 0.36926151
INFO:root:[  495] Training loss: 0.41898644, Validation loss: 0.42107243, Validation loss EMA: 0.37593598
INFO:root:[  500] Training loss: 0.41844989, Validation loss: 0.41175751, Validation loss EMA: 0.36709507
INFO:root:[  505] Training loss: 0.41829067, Validation loss: 0.41972985, Validation loss EMA: 0.37718126
INFO:root:[  510] Training loss: 0.41731383, Validation loss: 0.41358738, Validation loss EMA: 0.36733865
INFO:root:[  515] Training loss: 0.41744497, Validation loss: 0.41547725, Validation loss EMA: 0.37159081
INFO:root:[  520] Training loss: 0.41640838, Validation loss: 0.41870036, Validation loss EMA: 0.36779839
INFO:root:EP 524: Early stopping
INFO:root:After finishing all epochs: mem (CPU python)=3214.421875MB; mem (CPU total)=79400.15234375MB
INFO:root:Training the model took 794.999s.
INFO:root:Emptying the cuda cache took 0.001s.
INFO:root:Starting evaluation: model MLP & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 14.15687
INFO:root:RMSETrain: 3.76256
INFO:root:EnergyScoreTrain: 2.30642
INFO:root:CRPSTrain: 2.30008
INFO:root:Gaussian NLLTrain: 6.83366
INFO:root:CoverageTrain: 0.49651
INFO:root:QICETrain: 0.0898
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 14.11923
INFO:root:RMSEValidation: 3.75756
INFO:root:EnergyScoreValidation: 2.30224
INFO:root:CRPSValidation: 2.2959
INFO:root:Gaussian NLLValidation: 6.81317
INFO:root:CoverageValidation: 0.497
INFO:root:QICEValidation: 0.08967
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 17.79565
INFO:root:RMSETest: 4.21849
INFO:root:EnergyScoreTest: 2.72759
INFO:root:CRPSTest: 2.7211
INFO:root:Gaussian NLLTest: 8.20329
INFO:root:CoverageTest: 0.42335
INFO:root:QICETest: 0.10481
INFO:root:After validation: mem (CPU python)=3263.71484375MB; mem (CPU total)=79263.796875MB
