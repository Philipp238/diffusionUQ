INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:: mem (CPU python)=541.48046875MB; mem (CPU total)=63599.015625MB
INFO:root:############### Starting experiment with config file debug_kin8nm.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'kin8nm', 'yarin_gal_uci_split_indices': 0, 'max_dataset_size': 1000, 'standardize': True}
INFO:root:After loading the datasets: mem (CPU python)=548.46875MB; mem (CPU total)=63716.12890625MB
INFO:root:###1 out of 1 training parameter combinations ###
INFO:root:Training parameters: {'report_every': 5, 'seed': 1234, 'model': 'MLP', 'uncertainty_quantification': 'dropout', 'backbone': 'default', 'batch_size': 64, 'eval_batch_size': 16384, 'n_epochs': 1000, 'early_stopping': 50, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0.0, 'dropout': 0.1, 'hidden_dim': 64, 'n_layers': 2, 'distributional_method': 'mixednormal', 'concat_condition_diffusion': True, 'evaluate': True, 'x_T_sampling_method': 'CARD', 'conditional_free_guidance_training': True, 'regressor': None}
INFO:root:Using split-0 for UCI dataset kin8nm
INFO:root:After creating the dataloaders: mem (CPU python)=548.72265625MB; mem (CPU total)=63552.62109375MB
INFO:root:NumberParameters: 8961
INFO:root:GPU memory allocated: 2097152
INFO:root:After setting up the model: mem (CPU python)=2147.84375MB; mem (CPU total)=66737.5625MB
INFO:root:Training starts now.
INFO:root:[    5] Training loss: 0.62402579, Validation loss: 0.39772233, Validation loss EMA: 0.37849474
INFO:root:[   10] Training loss: 0.34963658, Validation loss: 0.30799231, Validation loss EMA: 0.28072846
INFO:root:[   15] Training loss: 0.28922545, Validation loss: 0.25994986, Validation loss EMA: 0.23088409
INFO:root:[   20] Training loss: 0.24702470, Validation loss: 0.22665578, Validation loss EMA: 0.20035787
INFO:root:[   25] Training loss: 0.21617801, Validation loss: 0.20497879, Validation loss EMA: 0.17338023
INFO:root:[   30] Training loss: 0.19655378, Validation loss: 0.19174877, Validation loss EMA: 0.15400213
INFO:root:[   35] Training loss: 0.18465127, Validation loss: 0.17834044, Validation loss EMA: 0.14057671
INFO:root:[   40] Training loss: 0.17467143, Validation loss: 0.16901137, Validation loss EMA: 0.13019235
INFO:root:[   45] Training loss: 0.16688646, Validation loss: 0.15589140, Validation loss EMA: 0.12232700
INFO:root:[   50] Training loss: 0.16026624, Validation loss: 0.15400571, Validation loss EMA: 0.11604395
INFO:root:[   55] Training loss: 0.15512302, Validation loss: 0.15007697, Validation loss EMA: 0.11067108
INFO:root:[   60] Training loss: 0.15095192, Validation loss: 0.14694610, Validation loss EMA: 0.10608679
INFO:root:[   65] Training loss: 0.14658494, Validation loss: 0.14238241, Validation loss EMA: 0.10232291
INFO:root:[   70] Training loss: 0.14355756, Validation loss: 0.13956532, Validation loss EMA: 0.09921855
INFO:root:[   75] Training loss: 0.14184496, Validation loss: 0.13484354, Validation loss EMA: 0.09668411
INFO:root:[   80] Training loss: 0.13848009, Validation loss: 0.13281152, Validation loss EMA: 0.09433926
INFO:root:[   85] Training loss: 0.13420527, Validation loss: 0.13113295, Validation loss EMA: 0.09191863
INFO:root:[   90] Training loss: 0.13122962, Validation loss: 0.13065054, Validation loss EMA: 0.08960053
INFO:root:[   95] Training loss: 0.13099952, Validation loss: 0.12581876, Validation loss EMA: 0.08798659
INFO:root:[  100] Training loss: 0.12795140, Validation loss: 0.12764041, Validation loss EMA: 0.08625543
INFO:root:[  105] Training loss: 0.12613293, Validation loss: 0.12445430, Validation loss EMA: 0.08487316
INFO:root:[  110] Training loss: 0.12558813, Validation loss: 0.12473971, Validation loss EMA: 0.08378425
INFO:root:[  115] Training loss: 0.12388628, Validation loss: 0.12481633, Validation loss EMA: 0.08247364
INFO:root:[  120] Training loss: 0.12362849, Validation loss: 0.11984861, Validation loss EMA: 0.08152117
INFO:root:[  125] Training loss: 0.12246263, Validation loss: 0.11930612, Validation loss EMA: 0.08019239
INFO:root:[  130] Training loss: 0.12068143, Validation loss: 0.11751164, Validation loss EMA: 0.07900208
INFO:root:[  135] Training loss: 0.11737397, Validation loss: 0.11446684, Validation loss EMA: 0.07780466
INFO:root:[  140] Training loss: 0.11813073, Validation loss: 0.11687429, Validation loss EMA: 0.07687075
INFO:root:[  145] Training loss: 0.11649619, Validation loss: 0.11594701, Validation loss EMA: 0.07598857
INFO:root:[  150] Training loss: 0.11589468, Validation loss: 0.11181357, Validation loss EMA: 0.07551450
INFO:root:[  155] Training loss: 0.11342982, Validation loss: 0.11030455, Validation loss EMA: 0.07465690
INFO:root:[  160] Training loss: 0.11312906, Validation loss: 0.11387762, Validation loss EMA: 0.07370668
INFO:root:[  165] Training loss: 0.11324292, Validation loss: 0.10953263, Validation loss EMA: 0.07313542
INFO:root:[  170] Training loss: 0.11112830, Validation loss: 0.10954127, Validation loss EMA: 0.07246393
INFO:root:[  175] Training loss: 0.11006180, Validation loss: 0.10785648, Validation loss EMA: 0.07168010
INFO:root:[  180] Training loss: 0.11032415, Validation loss: 0.10829516, Validation loss EMA: 0.07123812
INFO:root:[  185] Training loss: 0.10962649, Validation loss: 0.10791500, Validation loss EMA: 0.07048488
INFO:root:[  190] Training loss: 0.10833265, Validation loss: 0.10837598, Validation loss EMA: 0.07003375
INFO:root:[  195] Training loss: 0.10795489, Validation loss: 0.10565714, Validation loss EMA: 0.06936074
INFO:root:[  200] Training loss: 0.10790563, Validation loss: 0.10459850, Validation loss EMA: 0.06911913
INFO:root:[  205] Training loss: 0.10625540, Validation loss: 0.10548104, Validation loss EMA: 0.06852145
INFO:root:[  210] Training loss: 0.10591894, Validation loss: 0.10315177, Validation loss EMA: 0.06800867
INFO:root:[  215] Training loss: 0.10628897, Validation loss: 0.10335708, Validation loss EMA: 0.06779627
INFO:root:[  220] Training loss: 0.10528288, Validation loss: 0.10216001, Validation loss EMA: 0.06743059
INFO:root:[  225] Training loss: 0.10373554, Validation loss: 0.10172461, Validation loss EMA: 0.06688688
INFO:root:[  230] Training loss: 0.10470386, Validation loss: 0.10041188, Validation loss EMA: 0.06649096
INFO:root:[  235] Training loss: 0.10403653, Validation loss: 0.10181671, Validation loss EMA: 0.06632175
INFO:root:[  240] Training loss: 0.10363887, Validation loss: 0.09959132, Validation loss EMA: 0.06575216
INFO:root:[  245] Training loss: 0.10306027, Validation loss: 0.10236561, Validation loss EMA: 0.06557878
INFO:root:[  250] Training loss: 0.10192014, Validation loss: 0.10014319, Validation loss EMA: 0.06524418
INFO:root:[  255] Training loss: 0.10147352, Validation loss: 0.10308132, Validation loss EMA: 0.06468199
INFO:root:[  260] Training loss: 0.10002790, Validation loss: 0.09951171, Validation loss EMA: 0.06445352
INFO:root:[  265] Training loss: 0.10168262, Validation loss: 0.10255334, Validation loss EMA: 0.06426733
INFO:root:[  270] Training loss: 0.10078945, Validation loss: 0.09822397, Validation loss EMA: 0.06428227
INFO:root:[  275] Training loss: 0.10068554, Validation loss: 0.10048546, Validation loss EMA: 0.06381796
INFO:root:[  280] Training loss: 0.10012937, Validation loss: 0.09949622, Validation loss EMA: 0.06338993
INFO:root:[  285] Training loss: 0.09967421, Validation loss: 0.09746192, Validation loss EMA: 0.06317167
INFO:root:[  290] Training loss: 0.09988026, Validation loss: 0.09750880, Validation loss EMA: 0.06265977
INFO:root:[  295] Training loss: 0.09940205, Validation loss: 0.09784339, Validation loss EMA: 0.06272298
INFO:root:[  300] Training loss: 0.09937181, Validation loss: 0.09516412, Validation loss EMA: 0.06220044
INFO:root:[  305] Training loss: 0.09812415, Validation loss: 0.10044247, Validation loss EMA: 0.06187646
INFO:root:[  310] Training loss: 0.09751404, Validation loss: 0.09770406, Validation loss EMA: 0.06166917
INFO:root:[  315] Training loss: 0.09871488, Validation loss: 0.09686020, Validation loss EMA: 0.06133887
INFO:root:[  320] Training loss: 0.09800386, Validation loss: 0.09610250, Validation loss EMA: 0.06158845
INFO:root:[  325] Training loss: 0.09624437, Validation loss: 0.09665131, Validation loss EMA: 0.06124339
INFO:root:[  330] Training loss: 0.09627524, Validation loss: 0.09730223, Validation loss EMA: 0.06092724
INFO:root:[  335] Training loss: 0.09624448, Validation loss: 0.09612521, Validation loss EMA: 0.06095969
INFO:root:[  340] Training loss: 0.09542735, Validation loss: 0.09424168, Validation loss EMA: 0.06043339
INFO:root:[  345] Training loss: 0.09493367, Validation loss: 0.09649792, Validation loss EMA: 0.06019404
INFO:root:[  350] Training loss: 0.09540485, Validation loss: 0.09405903, Validation loss EMA: 0.06020924
INFO:root:[  355] Training loss: 0.09420386, Validation loss: 0.09466029, Validation loss EMA: 0.05983478
INFO:root:[  360] Training loss: 0.09563208, Validation loss: 0.09554958, Validation loss EMA: 0.05978110
INFO:root:[  365] Training loss: 0.09496109, Validation loss: 0.09627102, Validation loss EMA: 0.05947666
INFO:root:[  370] Training loss: 0.09425485, Validation loss: 0.09236307, Validation loss EMA: 0.05947224
INFO:root:[  375] Training loss: 0.09359796, Validation loss: 0.09223659, Validation loss EMA: 0.05919912
INFO:root:[  380] Training loss: 0.09456594, Validation loss: 0.09172485, Validation loss EMA: 0.05893336
INFO:root:[  385] Training loss: 0.09451322, Validation loss: 0.09010556, Validation loss EMA: 0.05891250
INFO:root:[  390] Training loss: 0.09318659, Validation loss: 0.09378289, Validation loss EMA: 0.05875917
INFO:root:[  395] Training loss: 0.09426743, Validation loss: 0.09348213, Validation loss EMA: 0.05865574
INFO:root:[  400] Training loss: 0.09398002, Validation loss: 0.09095036, Validation loss EMA: 0.05850467
INFO:root:[  405] Training loss: 0.09408398, Validation loss: 0.09221150, Validation loss EMA: 0.05848022
INFO:root:[  410] Training loss: 0.09375807, Validation loss: 0.09406585, Validation loss EMA: 0.05807421
INFO:root:[  415] Training loss: 0.09321744, Validation loss: 0.09111613, Validation loss EMA: 0.05820091
INFO:root:[  420] Training loss: 0.09168958, Validation loss: 0.09132347, Validation loss EMA: 0.05804936
INFO:root:[  425] Training loss: 0.09308653, Validation loss: 0.09156921, Validation loss EMA: 0.05792050
INFO:root:[  430] Training loss: 0.09266057, Validation loss: 0.09058970, Validation loss EMA: 0.05773908
INFO:root:EP 434: Early stopping
INFO:root:After finishing all epochs: mem (CPU python)=3200.078125MB; mem (CPU total)=71029.1171875MB
INFO:root:Training the model took 152.811s.
INFO:root:Emptying the cuda cache took 0.0s.
INFO:root:Starting evaluation: model MLP & uncertainty quantification dropout
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 0.00417
INFO:root:RMSETrain: 0.06456
INFO:root:EnergyScoreTrain: 0.03671
INFO:root:CRPSTrain: 0.03645
INFO:root:Gaussian NLLTrain: -1.13245
INFO:root:CoverageTrain: 0.80618
INFO:root:QICETrain: 0.03489
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 0.0042
INFO:root:RMSEValidation: 0.06478
INFO:root:EnergyScoreValidation: 0.03671
INFO:root:CRPSValidation: 0.03646
INFO:root:Gaussian NLLValidation: -1.13753
INFO:root:CoverageValidation: 0.80728
INFO:root:QICEValidation: 0.03403
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 0.00541
INFO:root:RMSETest: 0.07354
INFO:root:EnergyScoreTest: 0.04278
INFO:root:CRPSTest: 0.04251
INFO:root:Gaussian NLLTest: -0.85132
INFO:root:CoverageTest: 0.76068
INFO:root:QICETest: 0.04818
INFO:root:After validation: mem (CPU python)=3227.984375MB; mem (CPU total)=70516.03125MB
