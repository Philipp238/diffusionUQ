INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:: mem (CPU python)=520.015625MB; mem (CPU total)=5574.56640625MB
INFO:root:############### Starting experiment with config file debug.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'power', 'max_dataset_size': 100000, 'standardize': False}
INFO:root:After loading the datasets: mem (CPU python)=529.4296875MB; mem (CPU total)=5579.01171875MB
INFO:root:###1 out of 1 training parameter combinations ###
INFO:root:Training parameters: {'report_every': 5, 'seed': 1234, 'model': 'MLP', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'eval_batch_size': 16384, 'n_epochs': 10000, 'early_stopping': 50, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0.0, 'dropout': 0.1, 'hidden_dim': 64, 'n_layers': 2, 'concat_condition_diffusion': True, 'evaluate': False, 'regressor': None}
INFO:root:After creating the dataloaders: mem (CPU python)=531.36328125MB; mem (CPU total)=5579.50390625MB
INFO:root:NumberParameters: 8705
INFO:root:GPU memory allocated: 2097152
INFO:root:After setting up the model: mem (CPU python)=625.62890625MB; mem (CPU total)=5657.87890625MB
INFO:root:Training starts now.
INFO:root:[    5] Training loss: 0.99313626, Validation loss: 0.90150338, Validation loss EMA: 0.90517205
INFO:root:[   10] Training loss: 0.82414157, Validation loss: 0.55084896, Validation loss EMA: 0.54773426
INFO:root:[   15] Training loss: 0.42455935, Validation loss: 0.32080421, Validation loss EMA: 0.29886529
INFO:root:[   20] Training loss: 0.29486896, Validation loss: 0.26874071, Validation loss EMA: 0.25784612
INFO:root:[   25] Training loss: 0.25879559, Validation loss: 0.23963648, Validation loss EMA: 0.22934154
INFO:root:[   30] Training loss: 0.24095363, Validation loss: 0.22705135, Validation loss EMA: 0.21337254
INFO:root:[   35] Training loss: 0.22850078, Validation loss: 0.21938626, Validation loss EMA: 0.20242870
INFO:root:[   40] Training loss: 0.22005218, Validation loss: 0.20864119, Validation loss EMA: 0.19343890
INFO:root:[   45] Training loss: 0.21300067, Validation loss: 0.19883601, Validation loss EMA: 0.18619193
INFO:root:[   50] Training loss: 0.20611342, Validation loss: 0.18472131, Validation loss EMA: 0.17928877
INFO:root:[   55] Training loss: 0.19904158, Validation loss: 0.19041169, Validation loss EMA: 0.17122766
INFO:root:[   60] Training loss: 0.19217490, Validation loss: 0.18468210, Validation loss EMA: 0.16414057
INFO:root:[   65] Training loss: 0.18455383, Validation loss: 0.16670659, Validation loss EMA: 0.15707257
INFO:root:[   70] Training loss: 0.17702160, Validation loss: 0.16105527, Validation loss EMA: 0.14974053
INFO:root:[   75] Training loss: 0.17067005, Validation loss: 0.15664449, Validation loss EMA: 0.14287493
INFO:root:[   80] Training loss: 0.16304421, Validation loss: 0.15432771, Validation loss EMA: 0.13593672
INFO:root:[   85] Training loss: 0.15565556, Validation loss: 0.14242463, Validation loss EMA: 0.12905379
INFO:root:[   90] Training loss: 0.14773814, Validation loss: 0.13849336, Validation loss EMA: 0.12237117
INFO:root:[   95] Training loss: 0.14018172, Validation loss: 0.14028138, Validation loss EMA: 0.11482111
INFO:root:[  100] Training loss: 0.13447080, Validation loss: 0.13527560, Validation loss EMA: 0.10838810
INFO:root:[  105] Training loss: 0.12711440, Validation loss: 0.12098065, Validation loss EMA: 0.10239959
INFO:root:[  110] Training loss: 0.12089605, Validation loss: 0.11136760, Validation loss EMA: 0.09726192
INFO:root:[  115] Training loss: 0.11608507, Validation loss: 0.10748150, Validation loss EMA: 0.09272611
INFO:root:[  120] Training loss: 0.11051718, Validation loss: 0.10974698, Validation loss EMA: 0.08853097
INFO:root:[  125] Training loss: 0.10721749, Validation loss: 0.09689120, Validation loss EMA: 0.08488884
INFO:root:[  130] Training loss: 0.10364283, Validation loss: 0.09972390, Validation loss EMA: 0.08169270
INFO:root:[  135] Training loss: 0.09988966, Validation loss: 0.09762681, Validation loss EMA: 0.07903145
INFO:root:[  140] Training loss: 0.09480149, Validation loss: 0.09497313, Validation loss EMA: 0.07647508
INFO:root:[  145] Training loss: 0.09370398, Validation loss: 0.09339704, Validation loss EMA: 0.07462400
INFO:root:[  150] Training loss: 0.09186598, Validation loss: 0.09278071, Validation loss EMA: 0.07259679
INFO:root:[  155] Training loss: 0.08898874, Validation loss: 0.08359398, Validation loss EMA: 0.07102882
INFO:root:[  160] Training loss: 0.08661751, Validation loss: 0.08400666, Validation loss EMA: 0.06968287
INFO:root:[  165] Training loss: 0.08510116, Validation loss: 0.08468571, Validation loss EMA: 0.06814948
INFO:root:[  170] Training loss: 0.08444786, Validation loss: 0.08247544, Validation loss EMA: 0.06722802
INFO:root:[  175] Training loss: 0.08296586, Validation loss: 0.08136982, Validation loss EMA: 0.06655627
INFO:root:[  180] Training loss: 0.08244318, Validation loss: 0.08314890, Validation loss EMA: 0.06593613
INFO:root:[  185] Training loss: 0.08259748, Validation loss: 0.08223479, Validation loss EMA: 0.06554126
INFO:root:[  190] Training loss: 0.08105838, Validation loss: 0.08549251, Validation loss EMA: 0.06488157
INFO:root:[  195] Training loss: 0.07862323, Validation loss: 0.08535173, Validation loss EMA: 0.06471389
INFO:root:[  200] Training loss: 0.08007568, Validation loss: 0.08042136, Validation loss EMA: 0.06505480
INFO:root:[  205] Training loss: 0.07815123, Validation loss: 0.07664678, Validation loss EMA: 0.06447943
INFO:root:[  210] Training loss: 0.07899042, Validation loss: 0.07832605, Validation loss EMA: 0.06446774
INFO:root:[  215] Training loss: 0.07858705, Validation loss: 0.09238909, Validation loss EMA: 0.06415471
INFO:root:[  220] Training loss: 0.07762954, Validation loss: 0.07875588, Validation loss EMA: 0.06421317
INFO:root:[  225] Training loss: 0.07839560, Validation loss: 0.07664341, Validation loss EMA: 0.06397255
INFO:root:[  230] Training loss: 0.07814621, Validation loss: 0.08114633, Validation loss EMA: 0.06417017
INFO:root:[  235] Training loss: 0.07702924, Validation loss: 0.08171426, Validation loss EMA: 0.06360594
INFO:root:[  240] Training loss: 0.07702219, Validation loss: 0.07813630, Validation loss EMA: 0.06360649
INFO:root:[  245] Training loss: 0.07782217, Validation loss: 0.07887617, Validation loss EMA: 0.06371121
INFO:root:[  250] Training loss: 0.07707281, Validation loss: 0.07936724, Validation loss EMA: 0.06330586
INFO:root:[  255] Training loss: 0.07634831, Validation loss: 0.07402825, Validation loss EMA: 0.06357229
INFO:root:[  260] Training loss: 0.07594841, Validation loss: 0.07811626, Validation loss EMA: 0.06302056
INFO:root:[  265] Training loss: 0.07563959, Validation loss: 0.07735364, Validation loss EMA: 0.06326601
INFO:root:[  270] Training loss: 0.07548543, Validation loss: 0.07490225, Validation loss EMA: 0.06320193
INFO:root:[  275] Training loss: 0.07476972, Validation loss: 0.07517908, Validation loss EMA: 0.06268875
INFO:root:[  280] Training loss: 0.07589685, Validation loss: 0.07906944, Validation loss EMA: 0.06265666
INFO:root:[  285] Training loss: 0.07550141, Validation loss: 0.07366310, Validation loss EMA: 0.06269196
INFO:root:[  290] Training loss: 0.07504248, Validation loss: 0.07327887, Validation loss EMA: 0.06285606
INFO:root:[  295] Training loss: 0.07459833, Validation loss: 0.07883338, Validation loss EMA: 0.06220126
INFO:root:[  300] Training loss: 0.07405497, Validation loss: 0.07608871, Validation loss EMA: 0.06218574
INFO:root:[  305] Training loss: 0.07495039, Validation loss: 0.07531004, Validation loss EMA: 0.06208527
INFO:root:[  310] Training loss: 0.07406322, Validation loss: 0.07801055, Validation loss EMA: 0.06182400
INFO:root:[  315] Training loss: 0.07464504, Validation loss: 0.07571094, Validation loss EMA: 0.06162915
INFO:root:[  320] Training loss: 0.07544583, Validation loss: 0.08398488, Validation loss EMA: 0.06174543
INFO:root:[  325] Training loss: 0.07461443, Validation loss: 0.07542520, Validation loss EMA: 0.06145236
INFO:root:[  330] Training loss: 0.07375092, Validation loss: 0.07336534, Validation loss EMA: 0.06174085
INFO:root:[  335] Training loss: 0.07379886, Validation loss: 0.07216764, Validation loss EMA: 0.06155627
INFO:root:[  340] Training loss: 0.07483401, Validation loss: 0.07638024, Validation loss EMA: 0.06147254
INFO:root:[  345] Training loss: 0.07317070, Validation loss: 0.07221825, Validation loss EMA: 0.06134635
INFO:root:[  350] Training loss: 0.07414655, Validation loss: 0.07411572, Validation loss EMA: 0.06108221
INFO:root:[  355] Training loss: 0.07316536, Validation loss: 0.08042184, Validation loss EMA: 0.06126089
INFO:root:[  360] Training loss: 0.07434358, Validation loss: 0.07553039, Validation loss EMA: 0.06128267
INFO:root:[  365] Training loss: 0.07371368, Validation loss: 0.07118378, Validation loss EMA: 0.06081652
INFO:root:[  370] Training loss: 0.07336147, Validation loss: 0.07738108, Validation loss EMA: 0.06074284
INFO:root:[  375] Training loss: 0.07326683, Validation loss: 0.07442430, Validation loss EMA: 0.06094430
INFO:root:[  380] Training loss: 0.07215691, Validation loss: 0.07847802, Validation loss EMA: 0.06053338
INFO:root:[  385] Training loss: 0.07407749, Validation loss: 0.07401139, Validation loss EMA: 0.06065486
INFO:root:[  390] Training loss: 0.07252913, Validation loss: 0.07355506, Validation loss EMA: 0.06074265
INFO:root:[  395] Training loss: 0.07244754, Validation loss: 0.07122216, Validation loss EMA: 0.06034367
INFO:root:[  400] Training loss: 0.07208630, Validation loss: 0.06981526, Validation loss EMA: 0.06090019
INFO:root:[  405] Training loss: 0.07247327, Validation loss: 0.07115474, Validation loss EMA: 0.06066907
INFO:root:[  410] Training loss: 0.07245614, Validation loss: 0.07226856, Validation loss EMA: 0.06057825
INFO:root:[  415] Training loss: 0.07240066, Validation loss: 0.09777315, Validation loss EMA: 0.06071153
INFO:root:[  420] Training loss: 0.07292694, Validation loss: 0.08849361, Validation loss EMA: 0.06044169
INFO:root:[  425] Training loss: 0.07236862, Validation loss: 0.06839136, Validation loss EMA: 0.06023129
INFO:root:[  430] Training loss: 0.07208918, Validation loss: 0.07039701, Validation loss EMA: 0.06020657
INFO:root:[  435] Training loss: 0.07240164, Validation loss: 0.07410088, Validation loss EMA: 0.06046009
INFO:root:[  440] Training loss: 0.07190343, Validation loss: 0.06993379, Validation loss EMA: 0.05991057
INFO:root:[  445] Training loss: 0.07193703, Validation loss: 0.07388735, Validation loss EMA: 0.06033657
INFO:root:[  450] Training loss: 0.07158597, Validation loss: 0.08269884, Validation loss EMA: 0.06000828
INFO:root:[  455] Training loss: 0.07246476, Validation loss: 0.07610536, Validation loss EMA: 0.06026146
INFO:root:[  460] Training loss: 0.07243856, Validation loss: 0.07397609, Validation loss EMA: 0.05982939
INFO:root:[  465] Training loss: 0.07080433, Validation loss: 0.07777984, Validation loss EMA: 0.05985599
INFO:root:[  470] Training loss: 0.07194390, Validation loss: 0.07470880, Validation loss EMA: 0.06013863
INFO:root:EP 474: Early stopping
INFO:root:After finishing all epochs: mem (CPU python)=796.07421875MB; mem (CPU total)=5798.71875MB
INFO:root:Training the model took 175.542s.
INFO:root:Emptying the cuda cache took 0.0s.
