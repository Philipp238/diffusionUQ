INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:: mem (CPU python)=520.1640625MB; mem (CPU total)=5594.4140625MB
INFO:root:############### Starting experiment with config file debug.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'housing_prices', 'max_dataset_size': 100000, 'standardize': True}
INFO:root:After loading the datasets: mem (CPU python)=563.52734375MB; mem (CPU total)=5620.71875MB
INFO:root:###1 out of 1 training parameter combinations ###
INFO:root:Training parameters: {'report_every': 5, 'seed': 1234, 'model': 'MLP', 'uncertainty_quantification': 'dropout', 'batch_size': 64, 'eval_batch_size': 16384, 'n_epochs': 10000, 'early_stopping': 50, 'init': 'default', 'learning_rate': 0.0001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0.0, 'dropout': 0.1, 'hidden_dim': 64, 'n_layers': 2, 'concat_condition_diffusion': True, 'evaluate': False, 'regressor': None}
INFO:root:After creating the dataloaders: mem (CPU python)=565.7890625MB; mem (CPU total)=5621.703125MB
INFO:root:NumberParameters: 8961
INFO:root:GPU memory allocated: 2097152
INFO:root:After setting up the model: mem (CPU python)=658.0MB; mem (CPU total)=5701.921875MB
INFO:root:Training starts now.
INFO:root:[    5] Training loss: 0.99631984, Validation loss: 1.02290118, Validation loss EMA: 1.02240980
INFO:root:[   10] Training loss: 0.99492779, Validation loss: 1.02321374, Validation loss EMA: 1.02113163
INFO:root:[   15] Training loss: 0.99384180, Validation loss: 1.02073455, Validation loss EMA: 1.01981664
INFO:root:[   20] Training loss: 0.99280449, Validation loss: 1.01928496, Validation loss EMA: 1.01846290
INFO:root:[   25] Training loss: 0.99080773, Validation loss: 1.01488566, Validation loss EMA: 1.01519346
INFO:root:[   30] Training loss: 0.98678160, Validation loss: 1.00897193, Validation loss EMA: 1.00887454
INFO:root:[   35] Training loss: 0.97910687, Validation loss: 0.99946004, Validation loss EMA: 0.99853396
INFO:root:[   40] Training loss: 0.96602567, Validation loss: 0.98640829, Validation loss EMA: 0.98176551
INFO:root:[   45] Training loss: 0.95096022, Validation loss: 0.96966726, Validation loss EMA: 0.96212971
INFO:root:[   50] Training loss: 0.93543152, Validation loss: 0.95330405, Validation loss EMA: 0.94392693
INFO:root:[   55] Training loss: 0.91853605, Validation loss: 0.92564631, Validation loss EMA: 0.92333490
INFO:root:[   60] Training loss: 0.90008034, Validation loss: 0.93237108, Validation loss EMA: 0.90017241
INFO:root:[   65] Training loss: 0.87749710, Validation loss: 0.89897370, Validation loss EMA: 0.86903065
INFO:root:[   70] Training loss: 0.84699383, Validation loss: 0.83943731, Validation loss EMA: 0.82788593
INFO:root:[   75] Training loss: 0.80941778, Validation loss: 0.78553867, Validation loss EMA: 0.77607048
INFO:root:[   80] Training loss: 0.75716131, Validation loss: 0.74670231, Validation loss EMA: 0.71410197
INFO:root:[   85] Training loss: 0.70886704, Validation loss: 0.67605180, Validation loss EMA: 0.65747428
INFO:root:[   90] Training loss: 0.67123504, Validation loss: 0.64174956, Validation loss EMA: 0.61631310
INFO:root:[   95] Training loss: 0.64449694, Validation loss: 0.65453750, Validation loss EMA: 0.58881986
INFO:root:[  100] Training loss: 0.62926777, Validation loss: 0.61784083, Validation loss EMA: 0.57419658
INFO:root:[  105] Training loss: 0.62176363, Validation loss: 0.65085548, Validation loss EMA: 0.56284446
INFO:root:[  110] Training loss: 0.60795313, Validation loss: 0.56730282, Validation loss EMA: 0.54969525
INFO:root:[  115] Training loss: 0.59832161, Validation loss: 0.57316804, Validation loss EMA: 0.54064000
INFO:root:[  120] Training loss: 0.59136859, Validation loss: 0.57205623, Validation loss EMA: 0.53431928
INFO:root:[  125] Training loss: 0.58529992, Validation loss: 0.54697859, Validation loss EMA: 0.52540958
INFO:root:[  130] Training loss: 0.57878058, Validation loss: 0.55536747, Validation loss EMA: 0.52125156
INFO:root:[  135] Training loss: 0.57070330, Validation loss: 0.60770679, Validation loss EMA: 0.51243496
INFO:root:[  140] Training loss: 0.56976604, Validation loss: 0.53631407, Validation loss EMA: 0.50730675
INFO:root:[  145] Training loss: 0.56183509, Validation loss: 0.55206454, Validation loss EMA: 0.50106812
INFO:root:[  150] Training loss: 0.55514667, Validation loss: 0.51330638, Validation loss EMA: 0.49878451
INFO:root:[  155] Training loss: 0.55113730, Validation loss: 0.56560105, Validation loss EMA: 0.49134997
INFO:root:[  160] Training loss: 0.54424967, Validation loss: 0.59394044, Validation loss EMA: 0.48662013
INFO:root:[  165] Training loss: 0.54335297, Validation loss: 0.50096738, Validation loss EMA: 0.48280919
INFO:root:[  170] Training loss: 0.53859450, Validation loss: 0.53504705, Validation loss EMA: 0.47583455
INFO:root:[  175] Training loss: 0.53394960, Validation loss: 0.59470540, Validation loss EMA: 0.47078425
INFO:root:[  180] Training loss: 0.52870000, Validation loss: 0.52036458, Validation loss EMA: 0.46943730
INFO:root:[  185] Training loss: 0.52641066, Validation loss: 0.47968027, Validation loss EMA: 0.46274635
INFO:root:[  190] Training loss: 0.52195607, Validation loss: 0.48109329, Validation loss EMA: 0.46108589
INFO:root:[  195] Training loss: 0.51855138, Validation loss: 0.49158630, Validation loss EMA: 0.45595095
INFO:root:[  200] Training loss: 0.51341432, Validation loss: 0.47451138, Validation loss EMA: 0.45166114
INFO:root:[  205] Training loss: 0.51534745, Validation loss: 0.47896427, Validation loss EMA: 0.44941330
INFO:root:[  210] Training loss: 0.50697535, Validation loss: 0.49985072, Validation loss EMA: 0.44449165
INFO:root:[  215] Training loss: 0.50276041, Validation loss: 0.46203759, Validation loss EMA: 0.44205770
INFO:root:[  220] Training loss: 0.50427353, Validation loss: 0.49992234, Validation loss EMA: 0.43829972
INFO:root:[  225] Training loss: 0.50016058, Validation loss: 0.45778868, Validation loss EMA: 0.43457761
INFO:root:[  230] Training loss: 0.49648877, Validation loss: 0.46487337, Validation loss EMA: 0.43250641
INFO:root:[  235] Training loss: 0.49522187, Validation loss: 0.46602044, Validation loss EMA: 0.43130955
INFO:root:[  240] Training loss: 0.48979976, Validation loss: 0.48905140, Validation loss EMA: 0.42845166
INFO:root:[  245] Training loss: 0.48979789, Validation loss: 0.44892552, Validation loss EMA: 0.42561588
INFO:root:[  250] Training loss: 0.48786907, Validation loss: 0.47515774, Validation loss EMA: 0.42306307
INFO:root:[  255] Training loss: 0.48019700, Validation loss: 0.45365039, Validation loss EMA: 0.41929621
INFO:root:[  260] Training loss: 0.48783552, Validation loss: 0.43019706, Validation loss EMA: 0.41851163
INFO:root:[  265] Training loss: 0.48020962, Validation loss: 0.48129505, Validation loss EMA: 0.41626501
INFO:root:[  270] Training loss: 0.47379292, Validation loss: 0.47556904, Validation loss EMA: 0.41012183
INFO:root:[  275] Training loss: 0.47296135, Validation loss: 0.45934594, Validation loss EMA: 0.40817502
INFO:root:[  280] Training loss: 0.47454664, Validation loss: 0.47328410, Validation loss EMA: 0.40622932
INFO:root:[  285] Training loss: 0.46918727, Validation loss: 0.42956400, Validation loss EMA: 0.40194499
INFO:root:[  290] Training loss: 0.46323722, Validation loss: 0.42556038, Validation loss EMA: 0.39690995
INFO:root:[  295] Training loss: 0.45957960, Validation loss: 0.45945653, Validation loss EMA: 0.39109623
INFO:root:[  300] Training loss: 0.45613847, Validation loss: 0.41805214, Validation loss EMA: 0.38712344
INFO:root:[  305] Training loss: 0.45353882, Validation loss: 0.42139435, Validation loss EMA: 0.38313338
INFO:root:[  310] Training loss: 0.45248805, Validation loss: 0.42810920, Validation loss EMA: 0.38082808
INFO:root:[  315] Training loss: 0.44892046, Validation loss: 0.40877351, Validation loss EMA: 0.37751192
INFO:root:[  320] Training loss: 0.44778303, Validation loss: 0.39928246, Validation loss EMA: 0.37612233
INFO:root:[  325] Training loss: 0.44354720, Validation loss: 0.44497171, Validation loss EMA: 0.37404644
INFO:root:[  330] Training loss: 0.44420875, Validation loss: 0.40669003, Validation loss EMA: 0.37065786
INFO:root:[  335] Training loss: 0.44123648, Validation loss: 0.39857650, Validation loss EMA: 0.36645156
INFO:root:[  340] Training loss: 0.43901846, Validation loss: 0.38866892, Validation loss EMA: 0.36550277
INFO:root:[  345] Training loss: 0.43585625, Validation loss: 0.41428292, Validation loss EMA: 0.36352605
INFO:root:[  350] Training loss: 0.43688439, Validation loss: 0.38676667, Validation loss EMA: 0.36358383
INFO:root:[  355] Training loss: 0.43694489, Validation loss: 0.42521006, Validation loss EMA: 0.36235589
INFO:root:[  360] Training loss: 0.43507974, Validation loss: 0.39231789, Validation loss EMA: 0.36222526
INFO:root:[  365] Training loss: 0.43167671, Validation loss: 0.38387135, Validation loss EMA: 0.35740152
INFO:root:[  370] Training loss: 0.43029522, Validation loss: 0.38005546, Validation loss EMA: 0.35531729
INFO:root:[  375] Training loss: 0.43221424, Validation loss: 0.40480161, Validation loss EMA: 0.35661015
INFO:root:[  380] Training loss: 0.42977734, Validation loss: 0.41390848, Validation loss EMA: 0.35649100
INFO:root:[  385] Training loss: 0.43005362, Validation loss: 0.37593061, Validation loss EMA: 0.35242608
INFO:root:[  390] Training loss: 0.42927292, Validation loss: 0.45358118, Validation loss EMA: 0.35299736
INFO:root:[  395] Training loss: 0.42674053, Validation loss: 0.37885243, Validation loss EMA: 0.35018092
INFO:root:[  400] Training loss: 0.42851350, Validation loss: 0.38275108, Validation loss EMA: 0.35002226
INFO:root:[  405] Training loss: 0.42764925, Validation loss: 0.37322789, Validation loss EMA: 0.34900549
INFO:root:[  410] Training loss: 0.42342403, Validation loss: 0.37625322, Validation loss EMA: 0.34848928
INFO:root:[  415] Training loss: 0.42431269, Validation loss: 0.37452331, Validation loss EMA: 0.34669748
INFO:root:[  420] Training loss: 0.42257538, Validation loss: 0.40995416, Validation loss EMA: 0.34467646
INFO:root:[  425] Training loss: 0.42006216, Validation loss: 0.37773871, Validation loss EMA: 0.34426650
INFO:root:[  430] Training loss: 0.41754083, Validation loss: 0.36471149, Validation loss EMA: 0.34377950
INFO:root:[  435] Training loss: 0.41525786, Validation loss: 0.36594909, Validation loss EMA: 0.34266111
INFO:root:[  440] Training loss: 0.41811628, Validation loss: 0.39059025, Validation loss EMA: 0.34694985
INFO:root:[  445] Training loss: 0.41847775, Validation loss: 0.39524046, Validation loss EMA: 0.34027264
INFO:root:[  450] Training loss: 0.41906465, Validation loss: 0.37957788, Validation loss EMA: 0.34024486
INFO:root:[  455] Training loss: 0.41542746, Validation loss: 0.37772179, Validation loss EMA: 0.33966702
INFO:root:[  460] Training loss: 0.41974368, Validation loss: 0.36832887, Validation loss EMA: 0.34141695
INFO:root:[  465] Training loss: 0.41386405, Validation loss: 0.42682576, Validation loss EMA: 0.34004006
INFO:root:[  470] Training loss: 0.41690948, Validation loss: 0.37850934, Validation loss EMA: 0.33726335
INFO:root:[  475] Training loss: 0.41237233, Validation loss: 0.41892961, Validation loss EMA: 0.33792147
INFO:root:[  480] Training loss: 0.41071577, Validation loss: 0.36450994, Validation loss EMA: 0.33915073
INFO:root:[  485] Training loss: 0.41189593, Validation loss: 0.39776030, Validation loss EMA: 0.33670911
INFO:root:[  490] Training loss: 0.41136411, Validation loss: 0.39531097, Validation loss EMA: 0.33320725
INFO:root:[  495] Training loss: 0.41743675, Validation loss: 0.37143496, Validation loss EMA: 0.33388469
INFO:root:[  500] Training loss: 0.41181449, Validation loss: 0.35881850, Validation loss EMA: 0.33316118
INFO:root:[  505] Training loss: 0.40937734, Validation loss: 0.36044222, Validation loss EMA: 0.33237746
INFO:root:[  510] Training loss: 0.40785499, Validation loss: 0.38730007, Validation loss EMA: 0.33164996
INFO:root:[  515] Training loss: 0.41349348, Validation loss: 0.37301511, Validation loss EMA: 0.33250821
INFO:root:[  520] Training loss: 0.40813178, Validation loss: 0.36201593, Validation loss EMA: 0.33138126
INFO:root:[  525] Training loss: 0.41030592, Validation loss: 0.38406590, Validation loss EMA: 0.33197749
INFO:root:[  530] Training loss: 0.41520984, Validation loss: 0.36533692, Validation loss EMA: 0.33080930
INFO:root:[  535] Training loss: 0.41002774, Validation loss: 0.35764042, Validation loss EMA: 0.32998931
INFO:root:[  540] Training loss: 0.40539838, Validation loss: 0.41350555, Validation loss EMA: 0.32971707
INFO:root:[  545] Training loss: 0.41111622, Validation loss: 0.35268039, Validation loss EMA: 0.32871127
INFO:root:[  550] Training loss: 0.40450824, Validation loss: 0.37136230, Validation loss EMA: 0.32918534
INFO:root:[  555] Training loss: 0.40801162, Validation loss: 0.37172809, Validation loss EMA: 0.32966992
INFO:root:[  560] Training loss: 0.40734564, Validation loss: 0.36242893, Validation loss EMA: 0.33018535
INFO:root:[  565] Training loss: 0.40435361, Validation loss: 0.36750752, Validation loss EMA: 0.32651260
INFO:root:[  570] Training loss: 0.40552711, Validation loss: 0.41075850, Validation loss EMA: 0.32531914
INFO:root:[  575] Training loss: 0.40273002, Validation loss: 0.35729203, Validation loss EMA: 0.32379302
INFO:root:[  580] Training loss: 0.40231111, Validation loss: 0.37723547, Validation loss EMA: 0.32330406
INFO:root:[  585] Training loss: 0.40055690, Validation loss: 0.37833244, Validation loss EMA: 0.32491249
INFO:root:[  590] Training loss: 0.40155658, Validation loss: 0.35489541, Validation loss EMA: 0.32469106
INFO:root:EP 594: Early stopping
INFO:root:After finishing all epochs: mem (CPU python)=830.71484375MB; mem (CPU total)=5850.18359375MB
INFO:root:Training the model took 436.493s.
INFO:root:Emptying the cuda cache took 0.0s.
