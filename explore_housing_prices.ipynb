{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from data.low_dimensional import RegressionDataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from utils import train_utils\n",
    "from models import MLP\n",
    "from torch import optim\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('Using cuda')\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print('Using cpu')\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "images = torch.tensor(housing.target, dtype=torch.float32).unsqueeze(-1)\n",
    "labels = torch.tensor(housing.data, dtype=torch.float32)\n",
    "\n",
    "image_dim, label_dim = 1, labels.shape[1]\n",
    "\n",
    "# dataset = TensorDataset(images, labels)\n",
    "dataset = RegressionDataset(images=images, labels=labels, standardize=True)\n",
    "\n",
    "# Load dataset\n",
    "# data = fetch_california_housing()\n",
    "# X, y = data.data, data.target\n",
    "\n",
    "# # Split into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Normalize features\n",
    "# # scaler = StandardScaler()\n",
    "# # X_train = scaler.fit_transform(X_train)\n",
    "# # X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "# X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# # Create data loaders\n",
    "# train_dataset = TensorDataset(y_train_tensor, X_train_tensor)\n",
    "# test_dataset = TensorDataset(y_test_tensor, X_test_tensor)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2.1296]),\n",
       " tensor([-0.2648, -0.2123, -0.2670, -0.2766,  0.2397, -0.2741, -0.2173, -0.4748]))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 128\n",
    "n_layers = 2\n",
    "dropout = 0.0\n",
    "\n",
    "report_every = 10\n",
    "early_stopping = 100\n",
    "weight_decay = 0.0\n",
    "learning_rate = 1e-4\n",
    "\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "model = MLP(target_dim=image_dim, conditioning_dim=label_dim, hidden_dim=hidden_dim, layers=n_layers, dropout=dropout).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=weight_decay)\n",
    "\n",
    "report_every = report_every\n",
    "early_stopper = train_utils.EarlyStopper(\n",
    "    patience=int(early_stopping / report_every),\n",
    "    min_delta=0.0001,\n",
    ")\n",
    "running_loss = 0\n",
    "\n",
    "training_loss_list = []\n",
    "validation_loss_list = []\n",
    "validation_loss_list_ema = []\n",
    "epochs = []\n",
    "\n",
    "best_loss = torch.inf\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "training_dataset, validation_dataset, test_dataset = random_split(dataset, [0.8, 0.1, 0.1])\n",
    "train_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   10] Training loss: 0.99828591, Validation loss: 0.98028085\n",
      "[   20] Training loss: 0.99310509, Validation loss: 0.97209858\n",
      "[   30] Training loss: 0.96868437, Validation loss: 0.92027974\n",
      "[   40] Training loss: 0.90877140, Validation loss: 0.84273174\n",
      "[   50] Training loss: 0.78278273, Validation loss: 0.70300977\n",
      "[   60] Training loss: 0.61729401, Validation loss: 0.53115863\n",
      "[   70] Training loss: 0.53582132, Validation loss: 0.47654445\n",
      "[   80] Training loss: 0.50662096, Validation loss: 0.48233858\n",
      "[   90] Training loss: 0.49378373, Validation loss: 0.47523250\n",
      "[  100] Training loss: 0.48650760, Validation loss: 0.45663595\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    # logging.info(using(\"At the start of the epoch\"))\n",
    "\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "        predicted_images = model(labels)\n",
    "        loss = criterion(images, predicted_images)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if epoch % report_every == report_every - 1:\n",
    "        epochs.append(epoch)\n",
    "        model.eval()\n",
    "\n",
    "        validation_loss = 0\n",
    "        validation_loss_ema = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                sampled_images = model(labels)\n",
    "                \n",
    "                validation_loss += criterion(sampled_images, images).item()\n",
    "        \n",
    "        validation_loss_list.append(\n",
    "            validation_loss / len(val_loader)\n",
    "        )\n",
    "        validation_loss_list_ema.append(\n",
    "            validation_loss_ema / len(val_loader)\n",
    "        )\n",
    "        training_loss_list.append(\n",
    "            running_loss / report_every / (len(train_loader))\n",
    "        )\n",
    "        running_loss = 0.0\n",
    "\n",
    "    \n",
    "            \n",
    "        if early_stopper.early_stop(validation_loss):\n",
    "                print(f\"EP {epoch}: Early stopping\")\n",
    "                break\n",
    "        \n",
    "        print(\n",
    "                f\"[{epoch + 1:5d}] Training loss: {training_loss_list[-1]:.8f}, Validation loss: \"\n",
    "                f\"{validation_loss_list[-1]:.8f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_batch.shape: torch.Size([256, 8])\n",
      "Epoch 10/100, Loss: 1.4868\n",
      "Epoch 20/100, Loss: 1.4235\n",
      "Epoch 30/100, Loss: 1.1153\n",
      "Epoch 40/100, Loss: 1.0950\n",
      "Epoch 50/100, Loss: 0.7108\n",
      "Epoch 60/100, Loss: 0.6226\n",
      "Epoch 70/100, Loss: 0.6128\n",
      "Epoch 80/100, Loss: 0.6201\n",
      "Epoch 90/100, Loss: 0.5969\n",
      "Epoch 100/100, Loss: 0.6481\n",
      "Test MSE: 0.6072\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the MLP model\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 1)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = MLP(X_train.shape[1]).to(device)\n",
    "model = MLP(target_dim=image_dim, conditioning_dim=label_dim, hidden_dim=hidden_dim, layers=n_layers, dropout=dropout).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "first_time = True\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        # X_batch, y_batch = y_batch.to(device), X_batch.to(device)\n",
    "        if first_time:\n",
    "            print(f'X_batch.shape: {X_batch.shape}')\n",
    "            first_time = False\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        # X_batch, y_batch = y_batch.to(device), X_batch.to(device)\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "print(f\"Test MSE: {total_loss / len(test_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16512]), torch.Size([132096, 1]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.shape, y_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16512,), (16512, 8), (20640,), (20640, 8))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[110]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mTensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/groups/ai/scholl/miniconda3/envs/pfno2/lib/python3.12/site-packages/torch/utils/data/dataset.py:203\u001b[39m, in \u001b[36mTensorDataset.__init__\u001b[39m\u001b[34m(self, *tensors)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *tensors: Tensor) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[32m    204\u001b[39m         tensors[\u001b[32m0\u001b[39m].size(\u001b[32m0\u001b[39m) == tensor.size(\u001b[32m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors\n\u001b[32m    205\u001b[39m     ), \u001b[33m\"\u001b[39m\u001b[33mSize mismatch between tensors\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28mself\u001b[39m.tensors = tensors\n",
      "\u001b[31mAssertionError\u001b[39m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "TensorDataset(X_train_tensor, y_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (8) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     81\u001b[39m t = torch.randint(\u001b[32m0\u001b[39m, timesteps, (X_batch.shape[\u001b[32m0\u001b[39m],), device=device)\n\u001b[32m     82\u001b[39m noise = torch.randn_like(X_batch)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m X_noisy = \u001b[43mforward_diffusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_bar\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m optimizer.zero_grad()\n\u001b[32m     86\u001b[39m predicted_noise = model(X_noisy, t.float())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mforward_diffusion\u001b[39m\u001b[34m(x, noise, alpha_bar)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_diffusion\u001b[39m(x, noise, alpha_bar):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha_bar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m + torch.sqrt(\u001b[32m1\u001b[39m - alpha_bar) * noise\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (32) must match the size of tensor b (8) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the Diffusion Model\n",
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, input_dim, timesteps=100):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "        self.timesteps = timesteps\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim + 1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        t = t.view(-1, 1).expand(-1, x.shape[1])\n",
    "        x_t = torch.cat([x, t], dim=1)\n",
    "        return self.model(x_t)\n",
    "\n",
    "# Diffusion process functions\n",
    "def forward_diffusion(x, noise, alpha_bar):\n",
    "    return torch.sqrt(alpha_bar) * x + torch.sqrt(1 - alpha_bar) * noise\n",
    "\n",
    "def generate_alpha_bar(timesteps):\n",
    "    beta = torch.linspace(0.0001, 0.02, timesteps)\n",
    "    alpha = 1 - beta\n",
    "    alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "    return alpha_bar\n",
    "\n",
    "# Initialize model and training components\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "timesteps = 100\n",
    "alpha_bar = generate_alpha_bar(timesteps).to(device)\n",
    "\n",
    "model = DiffusionModel(X_train.shape[1]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        t = torch.randint(0, timesteps, (X_batch.shape[0],), device=device)\n",
    "        noise = torch.randn_like(X_batch)\n",
    "        X_noisy = forward_diffusion(X_batch, noise, alpha_bar[t])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predicted_noise = model(X_noisy, t.float())\n",
    "        loss = criterion(predicted_noise, noise)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluate model\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        t = torch.randint(0, timesteps, (X_batch.shape[0],), device=device)\n",
    "        noise = torch.randn_like(X_batch)\n",
    "        X_noisy = forward_diffusion(X_batch, noise, alpha_bar[t])\n",
    "        predicted_noise = model(X_noisy, t.float())\n",
    "        loss = criterion(predicted_noise, noise)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "print(f\"Test Loss: {total_loss / len(test_loader):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfno2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
