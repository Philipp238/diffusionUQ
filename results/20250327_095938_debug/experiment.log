INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:: mem (CPU python)=569.0546875MB; mem (CPU total)=3106.51171875MB
INFO:root:############### Starting experiment with config file debug.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'uniform-regression', 'max_dataset_size': 4096}
INFO:root:After loading the datasets: mem (CPU python)=570.6171875MB; mem (CPU total)=3107.00390625MB
INFO:root:###1 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'seed': 1234, 'model': 'MLP', 'uncertainty_quantification': 'dropout', 'batch_size': 4096, 'n_epochs': 40, 'early_stopping': 10, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0.01, 'dropout': 0.001, 'hidden_dim': 64, 'n_layers': 3, 'concat_condition_diffusion': False}
INFO:root:After creating the dataloaders: mem (CPU python)=572.875MB; mem (CPU total)=3107.49609375MB
INFO:root:NumberParameters: 12673
INFO:root:GPU memory allocated: 2097152
INFO:root:After setting up the model: mem (CPU python)=667.1953125MB; mem (CPU total)=3189.77734375MB
INFO:root:Training starts now.
INFO:root:At the start of the epoch: mem (CPU python)=667.1953125MB; mem (CPU total)=3189.77734375MB
