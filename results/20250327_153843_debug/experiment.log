INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:: mem (CPU python)=519.2421875MB; mem (CPU total)=3498.33984375MB
INFO:root:############### Starting experiment with config file debug.ini ###############
INFO:root:###1 out of 1 data set parameter combinations ###
INFO:root:Data parameters: {'dataset_name': 'x-squared', 'max_dataset_size': 32768}
INFO:root:After loading the datasets: mem (CPU python)=522.640625MB; mem (CPU total)=3498.83203125MB
INFO:root:###1 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'report_every': 1, 'seed': 1234, 'model': 'MLP', 'uncertainty_quantification': 'diffusion', 'batch_size': 32768, 'n_epochs': 2, 'early_stopping': 100, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0.01, 'dropout': 0.001, 'hidden_dim': 64, 'n_layers': 3, 'concat_condition_diffusion': False}
INFO:root:After creating the dataloaders: mem (CPU python)=524.69140625MB; mem (CPU total)=3500.30859375MB
INFO:root:NumberParameters: 16961
INFO:root:GPU memory allocated: 2097152
INFO:root:After setting up the model: mem (CPU python)=617.96484375MB; mem (CPU total)=3584.05859375MB
INFO:root:Training starts now.
INFO:root:At the start of the epoch: mem (CPU python)=650.73046875MB; mem (CPU total)=3601.19140625MB
INFO:root:[    1] Training loss: 1.04168487, Validation loss: 5531.04199219, Valdiation loss EMA: 5809.88574219
INFO:root:At the start of the epoch: mem (CPU python)=910.0234375MB; mem (CPU total)=3808.328125MB
INFO:root:[    2] Training loss: 1.00270116, Validation loss: 4342.93603516, Valdiation loss EMA: 4403.43505859
INFO:root:After finishing all epochs: mem (CPU python)=939.4140625MB; mem (CPU total)=3837.7578125MB
INFO:root:Training the model took 4.875s.
INFO:root:Emptying the cuda cache took 0.001s.
INFO:root:Starting evaluation: model MLP & uncertainty quantification diffusion
INFO:root:Evaluating the model on Train data.
INFO:root:MSETrain: 658.73622
INFO:root:EnergyScoreTrain: 46.66253
INFO:root:CRPSTrain: 45.54172
INFO:root:Evaluating the model on Validation data.
INFO:root:MSEValidation: 654.4137
INFO:root:EnergyScoreValidation: 46.70437
INFO:root:CRPSValidation: 45.58199
INFO:root:Evaluating the model on Test data.
INFO:root:MSETest: 684.57678
INFO:root:EnergyScoreTest: 46.53498
INFO:root:CRPSTest: 45.41674
INFO:root:After validation: mem (CPU python)=984.86328125MB; mem (CPU total)=3887.75MB
INFO:root:###2 out of 9 training parameter combinations ###
INFO:root:Training parameters: {'report_every': 1, 'seed': 1234, 'model': 'MLP', 'uncertainty_quantification': 'diffusion', 'batch_size': 32768, 'n_epochs': 2, 'early_stopping': 100, 'init': 'default', 'learning_rate': 0.001, 'lr_schedule': 'step', 'optimizer': 'adam', 'gradient_clipping': 1, 'data_loader_pin_memory': False, 'data_loader_num_workers': 0, 'distributed_training': False, 'alpha': 0.05, 'n_samples_uq': 100, 'weight_decay': 0.01, 'dropout': 0.005, 'hidden_dim': 64, 'n_layers': 3, 'concat_condition_diffusion': False}
INFO:root:After creating the dataloaders: mem (CPU python)=984.86328125MB; mem (CPU total)=3887.74609375MB
INFO:root:NumberParameters: 16961
INFO:root:GPU memory allocated: 44040192
INFO:root:After setting up the model: mem (CPU python)=984.86328125MB; mem (CPU total)=3887.74609375MB
INFO:root:Training starts now.
INFO:root:At the start of the epoch: mem (CPU python)=984.86328125MB; mem (CPU total)=3887.2734375MB
INFO:root:[    1] Training loss: 1.04182923, Validation loss: 5527.88623047, Valdiation loss EMA: 5806.15576172
INFO:root:At the start of the epoch: mem (CPU python)=989.05078125MB; mem (CPU total)=3887.7265625MB
INFO:root:[    2] Training loss: 1.00316656, Validation loss: 4346.03906250, Valdiation loss EMA: 4406.90283203
INFO:root:After finishing all epochs: mem (CPU python)=989.05078125MB; mem (CPU total)=3887.74609375MB
INFO:root:Training the model took 4.394s.
INFO:root:Emptying the cuda cache took 0.001s.
INFO:root:Starting evaluation: model MLP & uncertainty quantification diffusion
INFO:root:Evaluating the model on Train data.
